{"cells":[{"metadata":{"papermill":{"duration":0.089069,"end_time":"2021-03-08T18:09:22.981918","exception":false,"start_time":"2021-03-08T18:09:22.892849","status":"completed"},"tags":[]},"cell_type":"markdown","source":" # Sentiment Analyse + Deep Learn - Amazon App "},{"metadata":{"papermill":{"duration":0.092865,"end_time":"2021-03-08T18:09:23.163282","exception":false,"start_time":"2021-03-08T18:09:23.070417","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Special Thanks\n\nBefore starting, I would like to thank two project writers that were fundamental to this project to be accomplished.\n[Usman Malik](https://twitter.com/usman_malikk), the writer of [Python for NLP: Multi-label Text Classification with Keras](https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/) and also [Thiago Panini](https://www.linkedin.com/in/thiago-panini/) that wrote this amazing golden \nmedal notebook [E-Commerce Sentiment Analysis: EDA + Viz + NLP](https://www.kaggle.com/thiagopanini/e-commerce-sentiment-analysis-eda-viz-nlp) here on Kaggle.\n\nFor sure you guys are not helping just myself! Thank you for sharing! üôèüèª"},{"metadata":{"papermill":{"duration":0.084379,"end_time":"2021-03-08T18:09:23.33374","exception":false,"start_time":"2021-03-08T18:09:23.249361","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n# 1 - Introduction"},{"metadata":{"papermill":{"duration":0.110626,"end_time":"2021-03-08T18:09:23.54148","exception":false,"start_time":"2021-03-08T18:09:23.430854","status":"completed"},"tags":[]},"cell_type":"markdown","source":"There is a lot of information available in the text format on the web. Reading that we can understand the sentiment about websites/products/brands in public tweets or in a review publicaed.\n\nHowever if you have the mission of getting and evaluating the users' feedback and you are going to live more thar 200 years, maybe you have enough time to read all reviews in the App's Stores. Otherwise, teach the computer to do it for you will save your life!\n\nThe objective of the project is to extract, manipulate and create a good deep learning model to understand the user's sentiments classifying texts sent by a human being into Positive, Neutral and Negative.\n\n\nHave fun :)\n"},{"metadata":{"papermill":{"duration":0.086818,"end_time":"2021-03-08T18:09:23.723712","exception":false,"start_time":"2021-03-08T18:09:23.636894","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"data\"></a>\n# 2 - Data "},{"metadata":{"papermill":{"duration":0.084837,"end_time":"2021-03-08T18:09:23.893806","exception":false,"start_time":"2021-03-08T18:09:23.808969","status":"completed"},"tags":[]},"cell_type":"markdown","source":"\nIn this project we will use the english reviews about Amazon Shopping App available on Apple Store and Playstore. We extracted the data using **Google_Play_Scraper** and **App_Store_Scraper API** and will be merged.\n\nThe DataFrame has 2 columns, the **'Review'** columns (str) and the **'Scores'** column (int) the rating that goes from 1 to 5.\n"},{"metadata":{"papermill":{"duration":0.08633,"end_time":"2021-03-08T18:09:24.066207","exception":false,"start_time":"2021-03-08T18:09:23.979877","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"methodology\"></a>\n# 3 - Methodology "},{"metadata":{"papermill":{"duration":0.08438,"end_time":"2021-03-08T18:09:24.237074","exception":false,"start_time":"2021-03-08T18:09:24.152694","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To find the best Multi-label Text Classification model for our data set, first we will preprocess our text. To do that we will build a Pipeline with functions for Regular Expressions, Stop Words Removal and Lemmatize Process. To have a good overall understanding about our data, we are going to plot the communs N Gram for each sentiment, plot the distribution of the sentiments and a Words Cloud for each sentiment.\nBefore modeling we will split in test and train, vectorize, balance the dataset, create a few functions for model evaluation so then we will create a neural network for a multilabel classification and finely we will create a function that uses the pipeline and apply the mode. So then, we will show the prediction and the percentages into any review added. \n\n* For manipulation and analyze the data we are going to use the **Pandas** and **Numpy** libraries.\n* For **Data Extraction** the data from Google Play and App Store store we are going to use **Google_Play_Scraper API** and **App_Dtore_Scraper API** respectively.\n* We will use **RE, Unidecode** and **NLTK** libraries for **Text Processing**.\n* For **lemmatization** we will nltk.stem library and the module  **WordNetLemmatizer** .\n* The **nltk.corpus** library we will use the module **Stopwords** to remove the english stop words.\n* For the **Pipeline** we are going to use **Sklearn** library, **Base Estimator** and **Transformer Mixin** modules.\n* For a nice and interactive plot we will use **make_subplots, graph_objects** and **express** modules from **Ploty** library.\n* For the **Word Cloud** we will use the module **WordCloud** from **Matplotlib.pyplot**.\n* To count the n grams for each sentiment we are going to use **sklearn.feature_extraction.text** librarie and  **CountVectorizer**  module.\n* For split in train and test we will use **train_test_split** module from **sklearn.model_selection** library. (As always)\n* For **Embedding** we will need **Tensorflow. Keras** library, **Tokenizer** and **Sequence Pad_Sequences** modules for applying the **GloVe** method.\n* For **Balancing** we will use from **imblearn**library and  **RandomOverSampler** module.\n* For **Feature selection**, we are going to use **Sklearn feature selection** library and  **F_classif** ,**Select K Best** modules\n* For **Evaluation Metrics** we use functions to **calculate manually** the **Precision, Recall** and **F1 score**, using **backends** module from **tensorflow Keras** library.\n* For the **Neural Network** we are going to use **tensorflow.Keras** library and modules **Model, Dense, LSTM, Input** and **Embedding.**\n* For plot the model we will use from **plot_model** module from **tensorflow.keras.utils** library.\n* Again to plot the history of the model on the training set, on the validation set and the on the final prediction labels we are going to use **matplotlib** library.\n \n\n\nLet's get started!\n\n\n"},{"metadata":{"papermill":{"duration":0.089362,"end_time":"2021-03-08T18:09:24.411605","exception":false,"start_time":"2021-03-08T18:09:24.322243","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"DE\"></a>\n# 4 - Data Extraction"},{"metadata":{"papermill":{"duration":0.085687,"end_time":"2021-03-08T18:09:24.583584","exception":false,"start_time":"2021-03-08T18:09:24.497897","status":"completed"},"tags":[]},"cell_type":"markdown","source":"First of all, we will scrape the reviews and the scores of Amazon from the apps stores for apple users and Play Store for android users.\n \n To do that we will use two api that make this process easy. Google_Play_Scraper API and App_Dtore_Scraper API.\n After scrapping\n \n After scraping the reviews and score, we will append the two dfs and check the data.\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:09:24.763531Z","iopub.status.busy":"2021-03-08T18:09:24.76278Z","iopub.status.idle":"2021-03-08T18:09:24.766033Z","shell.execute_reply":"2021-03-08T18:09:24.76661Z"},"papermill":{"duration":0.096529,"end_time":"2021-03-08T18:09:24.767011","exception":false,"start_time":"2021-03-08T18:09:24.670482","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.088014,"end_time":"2021-03-08T18:09:24.940082","exception":false,"start_time":"2021-03-08T18:09:24.852068","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"WS\"></a>\n## 4.1 - Web Scraping"},{"metadata":{"papermill":{"duration":0.091219,"end_time":"2021-03-08T18:09:25.117869","exception":false,"start_time":"2021-03-08T18:09:25.02665","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Play Store Scraper"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:09:25.296468Z","iopub.status.busy":"2021-03-08T18:09:25.295567Z","iopub.status.idle":"2021-03-08T18:09:37.413965Z","shell.execute_reply":"2021-03-08T18:09:37.414532Z"},"papermill":{"duration":12.210756,"end_time":"2021-03-08T18:09:37.414764","exception":false,"start_time":"2021-03-08T18:09:25.204008","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"! pip install google-play-scraper","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:09:37.599531Z","iopub.status.busy":"2021-03-08T18:09:37.598344Z","iopub.status.idle":"2021-03-08T18:09:37.610503Z","shell.execute_reply":"2021-03-08T18:09:37.609741Z"},"papermill":{"duration":0.106909,"end_time":"2021-03-08T18:09:37.610685","exception":false,"start_time":"2021-03-08T18:09:37.503776","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from google_play_scraper import Sort, reviews_all","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:09:37.797286Z","iopub.status.busy":"2021-03-08T18:09:37.796386Z","iopub.status.idle":"2021-03-08T18:09:53.590887Z","shell.execute_reply":"2021-03-08T18:09:53.591451Z"},"papermill":{"duration":15.89228,"end_time":"2021-03-08T18:09:53.591699","exception":false,"start_time":"2021-03-08T18:09:37.699419","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"result = reviews_all(\n    'com.amazon.mShop.android.shopping',\n    sleep_milliseconds=0.010, # defaults to 0\n    lang='en', # defaults to 'en'\n    country='us', # defaults to 'us'\n    sort=Sort.MOST_RELEVANT, # defaults to Sort.MOST_RELEVANT\n    filter_score_with= None # defaults to None(means all score)\n)\n\nresult = pd.DataFrame(result)\n\namazon_ps = pd.DataFrame()\namazon_ps['reviews']=result['content']\namazon_ps['score']= result['score']","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.088348,"end_time":"2021-03-08T18:09:53.768441","exception":false,"start_time":"2021-03-08T18:09:53.680093","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### App Store Scraper"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:09:53.952283Z","iopub.status.busy":"2021-03-08T18:09:53.951197Z","iopub.status.idle":"2021-03-08T18:10:02.548355Z","shell.execute_reply":"2021-03-08T18:10:02.548977Z"},"papermill":{"duration":8.690516,"end_time":"2021-03-08T18:10:02.549203","exception":false,"start_time":"2021-03-08T18:09:53.858687","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"! pip install app-store-scraper","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:10:02.743548Z","iopub.status.busy":"2021-03-08T18:10:02.742699Z","iopub.status.idle":"2021-03-08T18:10:02.749565Z","shell.execute_reply":"2021-03-08T18:10:02.750081Z"},"papermill":{"duration":0.10773,"end_time":"2021-03-08T18:10:02.750307","exception":false,"start_time":"2021-03-08T18:10:02.642577","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from app_store_scraper import AppStore","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:10:02.942223Z","iopub.status.busy":"2021-03-08T18:10:02.941371Z","iopub.status.idle":"2021-03-08T18:21:23.072346Z","shell.execute_reply":"2021-03-08T18:21:23.071329Z"},"papermill":{"duration":680.228385,"end_time":"2021-03-08T18:21:23.072595","exception":false,"start_time":"2021-03-08T18:10:02.84421","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"amazon = AppStore(country='us', app_name='amazon', app_id=297606951)\nresults = amazon.review(sleep=0.010)\n\nresults = pd.DataFrame(amazon.reviews)\n\namazon_as = pd.DataFrame()\namazon_as['reviews'] = results['review']\namazon_as['score'] = results['rating']","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:23.278912Z","iopub.status.busy":"2021-03-08T18:21:23.277811Z","iopub.status.idle":"2021-03-08T18:21:23.281783Z","shell.execute_reply":"2021-03-08T18:21:23.282356Z"},"papermill":{"duration":0.106628,"end_time":"2021-03-08T18:21:23.282594","exception":false,"start_time":"2021-03-08T18:21:23.175966","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# PlayStore\nprint(' Play Store')\nprint(\"Amazon - \",amazon_ps.shape[0])\n\n# Apple Store\nprint('\\n','Apple store')\nprint(\"Amazon - \",amazon_as.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:23.48352Z","iopub.status.busy":"2021-03-08T18:21:23.481565Z","iopub.status.idle":"2021-03-08T18:21:23.487111Z","shell.execute_reply":"2021-03-08T18:21:23.486309Z"},"papermill":{"duration":0.109157,"end_time":"2021-03-08T18:21:23.48729","exception":false,"start_time":"2021-03-08T18:21:23.378133","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Append\ndf = amazon_ps.append(amazon_as)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:23.687807Z","iopub.status.busy":"2021-03-08T18:21:23.686743Z","iopub.status.idle":"2021-03-08T18:21:23.691797Z","shell.execute_reply":"2021-03-08T18:21:23.691076Z"},"papermill":{"duration":0.108734,"end_time":"2021-03-08T18:21:23.691991","exception":false,"start_time":"2021-03-08T18:21:23.583257","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"print(\"Amazon - \",(f\"{df.shape[0]:,}\"))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.096645,"end_time":"2021-03-08T18:21:23.885053","exception":false,"start_time":"2021-03-08T18:21:23.788408","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"Dc\"></a>\n## 4.2 - Data Check"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:24.109585Z","iopub.status.busy":"2021-03-08T18:21:24.093247Z","iopub.status.idle":"2021-03-08T18:21:24.115491Z","shell.execute_reply":"2021-03-08T18:21:24.116115Z"},"papermill":{"duration":0.135059,"end_time":"2021-03-08T18:21:24.116373","exception":false,"start_time":"2021-03-08T18:21:23.981314","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:24.317839Z","iopub.status.busy":"2021-03-08T18:21:24.31704Z","iopub.status.idle":"2021-03-08T18:21:24.335655Z","shell.execute_reply":"2021-03-08T18:21:24.336252Z"},"papermill":{"duration":0.124448,"end_time":"2021-03-08T18:21:24.336476","exception":false,"start_time":"2021-03-08T18:21:24.212028","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:24.613219Z","iopub.status.busy":"2021-03-08T18:21:24.612389Z","iopub.status.idle":"2021-03-08T18:21:24.620538Z","shell.execute_reply":"2021-03-08T18:21:24.621093Z"},"papermill":{"duration":0.113088,"end_time":"2021-03-08T18:21:24.62132","exception":false,"start_time":"2021-03-08T18:21:24.508232","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"df['score'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.096569,"end_time":"2021-03-08T18:21:24.816104","exception":false,"start_time":"2021-03-08T18:21:24.719535","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"SS\"></a>\n## 4.3 - Score to Sentiment"},{"metadata":{"papermill":{"duration":0.100026,"end_time":"2021-03-08T18:21:25.013983","exception":false,"start_time":"2021-03-08T18:21:24.913957","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now we have the reviews and their respective scores. So now we will change the numbers to sentiment. \n* 1 and 2 to 'Negative'\n* 3 for 'Neutral' \n* 4 and 5 to 'Positive'"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:25.226856Z","iopub.status.busy":"2021-03-08T18:21:25.225706Z","iopub.status.idle":"2021-03-08T18:21:25.230039Z","shell.execute_reply":"2021-03-08T18:21:25.229078Z"},"papermill":{"duration":0.115293,"end_time":"2021-03-08T18:21:25.230252","exception":false,"start_time":"2021-03-08T18:21:25.114959","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def sentiment(df):\n    df['sentiment']='-'\n    df.loc[df['score']<=2,'sentiment']='negative'\n    df.loc[df['score']==3,'sentiment']='neutral'\n    df.loc[df['score']>=4,'sentiment']='positive'\n    \n    df = pd.get_dummies(df,columns=['sentiment']).reset_index(drop=True).drop(columns=['score'])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:25.444179Z","iopub.status.busy":"2021-03-08T18:21:25.442769Z","iopub.status.idle":"2021-03-08T18:21:25.493971Z","shell.execute_reply":"2021-03-08T18:21:25.493351Z"},"papermill":{"duration":0.165063,"end_time":"2021-03-08T18:21:25.494169","exception":false,"start_time":"2021-03-08T18:21:25.329106","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"df = sentiment(df)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.105735,"end_time":"2021-03-08T18:21:25.700302","exception":false,"start_time":"2021-03-08T18:21:25.594567","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"TP\"></a>\n# 5 - Text Processing Pipeline "},{"metadata":{"papermill":{"duration":0.10025,"end_time":"2021-03-08T18:21:25.898114","exception":false,"start_time":"2021-03-08T18:21:25.797864","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Text Preprocessing is an important step for natural language processing (NLP). This process will bring our reviews into a form that is predictable and analyzable for our neural network.\n\nFor a future verification of our model we will build a pipeline that contains all of the steps of our text preprocessing. So here we will build functions to lowercase the text and remove HTML, remove accented characters, extended contractions, remove special characters, lemmatization, removing stop words, checking if is an english word and removing digits and duplicates white spaces.\n"},{"metadata":{"papermill":{"duration":0.100177,"end_time":"2021-03-08T18:21:26.096071","exception":false,"start_time":"2021-03-08T18:21:25.995894","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"RHTML\"></a>\n## 5.1 - Lowercase and Remove HTML"},{"metadata":{"papermill":{"duration":0.096371,"end_time":"2021-03-08T18:21:26.295263","exception":false,"start_time":"2021-03-08T18:21:26.198892","status":"completed"},"tags":[]},"cell_type":"markdown","source":"For lowercase and remove the html as in almost all of the nexts functions we will use the Regular Expressions."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:26.515888Z","iopub.status.busy":"2021-03-08T18:21:26.515136Z","iopub.status.idle":"2021-03-08T18:21:26.518258Z","shell.execute_reply":"2021-03-08T18:21:26.518853Z"},"papermill":{"duration":0.124988,"end_time":"2021-03-08T18:21:26.519122","exception":false,"start_time":"2021-03-08T18:21:26.394134","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:26.756991Z","iopub.status.busy":"2021-03-08T18:21:26.756126Z","iopub.status.idle":"2021-03-08T18:21:26.764757Z","shell.execute_reply":"2021-03-08T18:21:26.763991Z"},"papermill":{"duration":0.137575,"end_time":"2021-03-08T18:21:26.764977","exception":false,"start_time":"2021-03-08T18:21:26.627402","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"TAG_RE = re.compile(r'<[^>]+>')\n\ndef re_tags(text_list): #define remove tag funtion\n    return [TAG_RE.sub('', str(word)).lower() for word in text_list]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.097949,"end_time":"2021-03-08T18:21:26.963995","exception":false,"start_time":"2021-03-08T18:21:26.866046","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"RAC\"></a>\n## 5.2 - Removing Accented Characters"},{"metadata":{"papermill":{"duration":0.09892,"end_time":"2021-03-08T18:21:27.164011","exception":false,"start_time":"2021-03-08T18:21:27.065091","status":"completed"},"tags":[]},"cell_type":"markdown","source":"For remove the accented characters we will use the module and unidecode"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:27.371746Z","iopub.status.busy":"2021-03-08T18:21:27.370727Z","iopub.status.idle":"2021-03-08T18:21:27.380018Z","shell.execute_reply":"2021-03-08T18:21:27.38067Z"},"papermill":{"duration":0.113103,"end_time":"2021-03-08T18:21:27.380944","exception":false,"start_time":"2021-03-08T18:21:27.267841","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import unidecode","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:27.583544Z","iopub.status.busy":"2021-03-08T18:21:27.582641Z","iopub.status.idle":"2021-03-08T18:21:27.584649Z","shell.execute_reply":"2021-03-08T18:21:27.58516Z"},"papermill":{"duration":0.107379,"end_time":"2021-03-08T18:21:27.585391","exception":false,"start_time":"2021-03-08T18:21:27.478012","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def re_accented_char(text_list):\n   \n    return [unidecode.unidecode(word.encode().decode('utf-8')) for word in text_list]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.098622,"end_time":"2021-03-08T18:21:27.78463","exception":false,"start_time":"2021-03-08T18:21:27.686008","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"EC\"></a>\n## 5.3 - Extended Contractions "},{"metadata":{"papermill":{"duration":0.097383,"end_time":"2021-03-08T18:21:27.983757","exception":false,"start_time":"2021-03-08T18:21:27.886374","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To extend the contractions. First we will replace possible mistakes to the right contraction, then we will use  the re.sub functions to extend then."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:28.191866Z","iopub.status.busy":"2021-03-08T18:21:28.19099Z","iopub.status.idle":"2021-03-08T18:21:28.202978Z","shell.execute_reply":"2021-03-08T18:21:28.203674Z"},"papermill":{"duration":0.121217,"end_time":"2021-03-08T18:21:28.203908","exception":false,"start_time":"2021-03-08T18:21:28.082691","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def ex_contractions(text_list):\n    result=[]\n    for word in text_list:\n        # replace contracting withoutsignal\n        word = word.replace(\"wont\",\"won't\")\n        word = word.replace(\"cant\",\"can't\")\n        word = word.replace(\"its\",\"it's\")\n        word = word.replace(\"youre\",\"you're\")\n        word = word.replace(\"hes\",\"he's\")\n        word = word.replace(\"shes\",\"she's\")\n        word = word.replace(\"its\",\"it's\")\n        word = word.replace(\"weare\",\"we're\")\n        word = word.replace(\"theyre\",\"they're\")\n\n        # specific\n        word = re.sub(r\"won\\'t\", \"will not\", str(word))\n        word = re.sub(r\"can\\'t\", \"can not\", str(word))\n\n        # general\n        word = re.sub(r\"n\\'t\", \" not\", str(word))\n        word = re.sub(r\"\\'re\", \" are\", str(word))\n        word = re.sub(r\"\\'s\", \" is\", str(word))\n        word = re.sub(r\"\\'d\", \" would\", str(word))\n        word = re.sub(r\"\\'ll\", \" will\", str(word))\n        word = re.sub(r\"\\'t\", \" not\", str(word))\n        word = re.sub(r\"\\'ve\", \" have\", str(word))\n        word = re.sub(r\"\\'m\", \" am\", str(word))\n        result.append(word)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.100249,"end_time":"2021-03-08T18:21:28.403529","exception":false,"start_time":"2021-03-08T18:21:28.30328","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"RSC\"></a>\n## 5.4 - Removing Special Characters"},{"metadata":{"papermill":{"duration":0.097021,"end_time":"2021-03-08T18:21:28.598702","exception":false,"start_time":"2021-03-08T18:21:28.501681","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To remove the special characters we will use the re.sub functions again."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:28.804335Z","iopub.status.busy":"2021-03-08T18:21:28.803532Z","iopub.status.idle":"2021-03-08T18:21:28.810692Z","shell.execute_reply":"2021-03-08T18:21:28.81006Z"},"papermill":{"duration":0.109739,"end_time":"2021-03-08T18:21:28.8109","exception":false,"start_time":"2021-03-08T18:21:28.701161","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def re_special_chars(text_list):\n    return [re.sub(\"[^a-zA-Z0-9]\",\" \",word) for word in text_list]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.099402,"end_time":"2021-03-08T18:21:29.009975","exception":false,"start_time":"2021-03-08T18:21:28.910573","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"L\"></a>\n## 5.5 - Lemmatization"},{"metadata":{"papermill":{"duration":0.099768,"end_time":"2021-03-08T18:21:29.213127","exception":false,"start_time":"2021-03-08T18:21:29.113359","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Lemmatization is the process of converting a word to its base form. \n\n>\"The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\"The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.'\n>\n>Jasmin Schreiber. [See more at machinelearningplus](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n\nTo do the Lemmatization we will use the NLTK libraries. In order to lemmatize we will create an instance of the WordNetLemmatizer() and call the lemmatize() function on each word."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:29.417667Z","iopub.status.busy":"2021-03-08T18:21:29.416564Z","iopub.status.idle":"2021-03-08T18:21:31.764162Z","shell.execute_reply":"2021-03-08T18:21:31.762497Z"},"papermill":{"duration":2.454719,"end_time":"2021-03-08T18:21:31.764388","exception":false,"start_time":"2021-03-08T18:21:29.309669","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:31.974075Z","iopub.status.busy":"2021-03-08T18:21:31.973291Z","iopub.status.idle":"2021-03-08T18:21:31.976761Z","shell.execute_reply":"2021-03-08T18:21:31.977315Z"},"papermill":{"duration":0.115349,"end_time":"2021-03-08T18:21:31.977554","exception":false,"start_time":"2021-03-08T18:21:31.862205","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def lemmatize_text(text_list):\n    wnl = WordNetLemmatizer()\n    lemmatizer_sentence = []  \n    tokenizer=nltk.tokenize.WhitespaceTokenizer()\n    for word in tokenizer.tokenize(text_list):\n        lemmatizer_sentence.append(wnl.lemmatize(word,'v'))\n        lemmatizer_sentence.append(\" \")\n    \n    return(\"\".join(lemmatizer_sentence))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.102399,"end_time":"2021-03-08T18:21:32.179234","exception":false,"start_time":"2021-03-08T18:21:32.076835","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"RSW\"></a>\n## 5.6 - Removing Stop Words"},{"metadata":{"papermill":{"duration":0.097216,"end_time":"2021-03-08T18:21:32.37551","exception":false,"start_time":"2021-03-08T18:21:32.278294","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To remove stop words from the reviews, we will tokenize the sentence and then remove the word if it exists in the list of stop words provided by NLTK."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:32.577911Z","iopub.status.busy":"2021-03-08T18:21:32.577037Z","iopub.status.idle":"2021-03-08T18:21:32.580836Z","shell.execute_reply":"2021-03-08T18:21:32.581472Z"},"papermill":{"duration":0.107173,"end_time":"2021-03-08T18:21:32.581721","exception":false,"start_time":"2021-03-08T18:21:32.474548","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import nltk","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:32.783905Z","iopub.status.busy":"2021-03-08T18:21:32.783147Z","iopub.status.idle":"2021-03-08T18:21:32.787109Z","shell.execute_reply":"2021-03-08T18:21:32.787637Z"},"papermill":{"duration":0.108504,"end_time":"2021-03-08T18:21:32.787864","exception":false,"start_time":"2021-03-08T18:21:32.67936","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:32.990497Z","iopub.status.busy":"2021-03-08T18:21:32.989579Z","iopub.status.idle":"2021-03-08T18:21:32.995693Z","shell.execute_reply":"2021-03-08T18:21:32.996273Z"},"papermill":{"duration":0.110552,"end_time":"2021-03-08T18:21:32.996527","exception":false,"start_time":"2021-03-08T18:21:32.885975","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def stopwords_text(text_list):\n    stop = stopwords.words('english')\n    sentence_without = []\n    tokenizer=nltk.tokenize.WhitespaceTokenizer()\n    for word in tokenizer.tokenize(text_list):\n        if word not in stop:\n            sentence_without.append(word)\n            sentence_without.append(\" \")\n            \n    return(\"\".join(sentence_without))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.099474,"end_time":"2021-03-08T18:21:33.195484","exception":false,"start_time":"2021-03-08T18:21:33.09601","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"CEW\"></a>\n## 5.7 - Checking If Is An English Word"},{"metadata":{"papermill":{"duration":0.097992,"end_time":"2021-03-08T18:21:33.395895","exception":false,"start_time":"2021-03-08T18:21:33.297903","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To check if the words are in the English dictionary we will use NLTK words corpus."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:33.600243Z","iopub.status.busy":"2021-03-08T18:21:33.599233Z","iopub.status.idle":"2021-03-08T18:21:33.786222Z","shell.execute_reply":"2021-03-08T18:21:33.78552Z"},"papermill":{"duration":0.293018,"end_time":"2021-03-08T18:21:33.786409","exception":false,"start_time":"2021-03-08T18:21:33.493391","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"words=set(nltk.corpus.words.words())\n\ndef word_check(text_list):\n    result=[]\n    for word in text_list:\n        if word.lower() in words:\n            result.append(word)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.098194,"end_time":"2021-03-08T18:21:33.982893","exception":false,"start_time":"2021-03-08T18:21:33.884699","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"RDDWS\"></a>\n## 5.8 -  Removing Digits And Duplicates White Spaces"},{"metadata":{"papermill":{"duration":0.099741,"end_time":"2021-03-08T18:21:34.182573","exception":false,"start_time":"2021-03-08T18:21:34.082832","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Last but not least we will use the re.sub again to remove the duplicates white spaces and the digits."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:34.387644Z","iopub.status.busy":"2021-03-08T18:21:34.386884Z","iopub.status.idle":"2021-03-08T18:21:34.390618Z","shell.execute_reply":"2021-03-08T18:21:34.391182Z"},"papermill":{"duration":0.1107,"end_time":"2021-03-08T18:21:34.391416","exception":false,"start_time":"2021-03-08T18:21:34.280716","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def re_whitespaces(text_list): \n    result=[]\n    for word in text_list:\n        word=(re.sub(r'\\d','dig',str(word))) #remove numbers \n        word = (re.sub(r'\\s+',' ', str(word))) #remove duplicates white spacces\n        result.append(word)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:34.601241Z","iopub.status.busy":"2021-03-08T18:21:34.600435Z","iopub.status.idle":"2021-03-08T18:21:34.60813Z","shell.execute_reply":"2021-03-08T18:21:34.607398Z"},"papermill":{"duration":0.117271,"end_time":"2021-03-08T18:21:34.608318","exception":false,"start_time":"2021-03-08T18:21:34.491047","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Removing blank comments\ndf = df[df['reviews']!='']","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.101014,"end_time":"2021-03-08T18:21:34.808069","exception":false,"start_time":"2021-03-08T18:21:34.707055","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"Pipeline\"></a>\n## 5.9 - Pipeline"},{"metadata":{"papermill":{"duration":0.112595,"end_time":"2021-03-08T18:21:35.023192","exception":false,"start_time":"2021-03-08T18:21:34.910597","status":"completed"},"tags":[]},"cell_type":"markdown","source":"For the pipeline we are going to define 3 classes to apply all the text preprocessing  previous functions on the reviews.\n The first Class will apply all the functions created to prepare the text into a regular expression. The second Class will remove all the the stop words and the last Class will lemmatizer the remnants words  of the previous\nprocesses\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:35.232157Z","iopub.status.busy":"2021-03-08T18:21:35.231185Z","iopub.status.idle":"2021-03-08T18:21:35.239266Z","shell.execute_reply":"2021-03-08T18:21:35.23858Z"},"papermill":{"duration":0.112827,"end_time":"2021-03-08T18:21:35.239475","exception":false,"start_time":"2021-03-08T18:21:35.126648","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:35.467856Z","iopub.status.busy":"2021-03-08T18:21:35.466306Z","iopub.status.idle":"2021-03-08T18:21:35.471443Z","shell.execute_reply":"2021-03-08T18:21:35.470753Z"},"papermill":{"duration":0.131322,"end_time":"2021-03-08T18:21:35.471622","exception":false,"start_time":"2021-03-08T18:21:35.3403","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Class for regular expressions application\nclass ApplyRegex(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, regex_transformers):\n        self.regex_transformers = regex_transformers\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Applying all regex functions in the regex_transformers dictionary\n        for regex_name, regex_function in self.regex_transformers.items():\n            X = regex_function(X)\n            \n        return X","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:35.678506Z","iopub.status.busy":"2021-03-08T18:21:35.677476Z","iopub.status.idle":"2021-03-08T18:21:35.680979Z","shell.execute_reply":"2021-03-08T18:21:35.680319Z"},"papermill":{"duration":0.110273,"end_time":"2021-03-08T18:21:35.681167","exception":false,"start_time":"2021-03-08T18:21:35.570894","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class StopWordsRemoval(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, text_stopwords):\n        self.text_stopwords = text_stopwords\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return [self.text_stopwords(comment) for comment in X]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:35.889892Z","iopub.status.busy":"2021-03-08T18:21:35.888665Z","iopub.status.idle":"2021-03-08T18:21:35.893386Z","shell.execute_reply":"2021-03-08T18:21:35.892536Z"},"papermill":{"duration":0.111086,"end_time":"2021-03-08T18:21:35.893579","exception":false,"start_time":"2021-03-08T18:21:35.782493","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class LemmatizeProcess(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, Lemmatize):\n        self.Lemmatizer = Lemmatize\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return [self.Lemmatizer(comment) for comment in X]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.099903,"end_time":"2021-03-08T18:21:36.092542","exception":false,"start_time":"2021-03-08T18:21:35.992639","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now we are going to define a dictionary for all the regular expressions functions and then define the text preprocessing pipeline with the classes above the defined."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:36.300389Z","iopub.status.busy":"2021-03-08T18:21:36.299252Z","iopub.status.idle":"2021-03-08T18:21:36.303081Z","shell.execute_reply":"2021-03-08T18:21:36.303647Z"},"papermill":{"duration":0.110437,"end_time":"2021-03-08T18:21:36.303864","exception":false,"start_time":"2021-03-08T18:21:36.193427","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Defining regex transformers to be applied\nregex_transformers = {\n    'remove_tags': re_tags,\n    'remove_accents': re_accented_char,\n    'decontracted': ex_contractions,\n    're_sc': re_special_chars,\n    'whitespaces': re_whitespaces\n}\n\n# Building a text prep pipeline\ntext_prep_pipeline = Pipeline([\n    ('regex', ApplyRegex(regex_transformers)),\n    ('stopwords', StopWordsRemoval(stopwords_text)),\n    ('lemmatize', LemmatizeProcess(lemmatize_text)),\n])\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.102169,"end_time":"2021-03-08T18:21:36.507566","exception":false,"start_time":"2021-03-08T18:21:36.405397","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now we can apply the pipeline into the reviews column."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:36.722626Z","iopub.status.busy":"2021-03-08T18:21:36.717298Z","iopub.status.idle":"2021-03-08T18:21:48.20488Z","shell.execute_reply":"2021-03-08T18:21:48.205504Z"},"papermill":{"duration":11.595932,"end_time":"2021-03-08T18:21:48.205741","exception":false,"start_time":"2021-03-08T18:21:36.609809","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"df['reviews'] = text_prep_pipeline.fit_transform(df[df.columns[:1]].values)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.101158,"end_time":"2021-03-08T18:21:48.406258","exception":false,"start_time":"2021-03-08T18:21:48.3051","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"Plot\"></a>\n# 6 - Plot"},{"metadata":{"papermill":{"duration":0.099671,"end_time":"2021-03-08T18:21:48.606454","exception":false,"start_time":"2021-03-08T18:21:48.506783","status":"completed"},"tags":[]},"cell_type":"markdown","source":"For a good understanding of the Amazon reviews we are going to plot the text in three different ways.\n \n   * First we will plot the distribution of the sentiments using pie graph express plot from plotly.  \n   * The second plot is the famous Word Cloud graph from the wordcloud library.\n   * The last plot  is a bar plot for the n-gram, which is a sequence of n words most common shown together. We will plot the 1 grams, 2 grams, 3 grams and 4 grams for each sentiment."},{"metadata":{"papermill":{"duration":0.100534,"end_time":"2021-03-08T18:21:48.807336","exception":false,"start_time":"2021-03-08T18:21:48.706802","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"SD\"></a>\n## 6. 1 - Sentiment Distribution"},{"metadata":{"papermill":{"duration":0.100425,"end_time":"2021-03-08T18:21:49.008274","exception":false,"start_time":"2021-03-08T18:21:48.907849","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To plot the distribution of the sentiments in our df we will use the main df to plot the total of each sentiment in a pie graph using the library plotly.express. "},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:49.219546Z","iopub.status.busy":"2021-03-08T18:21:49.218648Z","iopub.status.idle":"2021-03-08T18:21:50.306763Z","shell.execute_reply":"2021-03-08T18:21:50.306002Z"},"papermill":{"duration":1.194575,"end_time":"2021-03-08T18:21:50.306982","exception":false,"start_time":"2021-03-08T18:21:49.112407","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:50.521396Z","iopub.status.busy":"2021-03-08T18:21:50.520234Z","iopub.status.idle":"2021-03-08T18:21:50.524214Z","shell.execute_reply":"2021-03-08T18:21:50.523575Z"},"papermill":{"duration":0.115716,"end_time":"2021-03-08T18:21:50.52441","exception":false,"start_time":"2021-03-08T18:21:50.408694","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"total = pd.DataFrame(df[df.columns[1:]].sum()).rename(columns={0:'Total'})","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:50.73013Z","iopub.status.busy":"2021-03-08T18:21:50.729359Z","iopub.status.idle":"2021-03-08T18:21:52.362425Z","shell.execute_reply":"2021-03-08T18:21:52.361477Z"},"papermill":{"duration":1.738245,"end_time":"2021-03-08T18:21:52.362628","exception":false,"start_time":"2021-03-08T18:21:50.624383","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"fig = px.pie(total,values='Total',names=total.index)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.101049,"end_time":"2021-03-08T18:21:52.565659","exception":false,"start_time":"2021-03-08T18:21:52.46461","status":"completed"},"tags":[]},"cell_type":"markdown","source":"As expected is a very unbalanced data frame, since we define 2 scores (1 and 2) for negative and 2 scores for positive (4 and 5) and just one score ( 3 ) for neutral sentiments. Knowing that, we are going to balance the df in later actions before applying the neural networking.\n \n"},{"metadata":{"papermill":{"duration":0.107199,"end_time":"2021-03-08T18:21:52.774624","exception":false,"start_time":"2021-03-08T18:21:52.667425","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"WC\"></a>\n## 6.2 - Words Cloud"},{"metadata":{"papermill":{"duration":0.10258,"end_time":"2021-03-08T18:21:52.98087","exception":false,"start_time":"2021-03-08T18:21:52.87829","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now we are going to do the famous Word Cloud graph, an image composed of words used in the Amazon reviews, in which the size of each word indicates its frequency. \n\nTo do that we are going to create 3 df. Positive, Neutral and Negative with the words that compose the reviews of each sentiment and we are going to count how many times it repeat using values_count()\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:53.192017Z","iopub.status.busy":"2021-03-08T18:21:53.190949Z","iopub.status.idle":"2021-03-08T18:21:53.242369Z","shell.execute_reply":"2021-03-08T18:21:53.241634Z"},"papermill":{"duration":0.159524,"end_time":"2021-03-08T18:21:53.242564","exception":false,"start_time":"2021-03-08T18:21:53.08304","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:53.458221Z","iopub.status.busy":"2021-03-08T18:21:53.457251Z","iopub.status.idle":"2021-03-08T18:21:53.680072Z","shell.execute_reply":"2021-03-08T18:21:53.679387Z"},"papermill":{"duration":0.334541,"end_time":"2021-03-08T18:21:53.680245","exception":false,"start_time":"2021-03-08T18:21:53.345704","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"pos_comments = list(df[df['sentiment_positive']==1]['reviews'].values)\npositive_words = ' '.join(pos_comments).split(' ')\npositive_words = pd.DataFrame(positive_words,columns=['words'])['words'].value_counts()[1:]\n\nneu_comments = list(df[df['sentiment_neutral']==1]['reviews'].values)\nneutral_words = ' '.join(neu_comments).split(' ')\nneutral_words = pd.DataFrame(neutral_words,columns=['words'])['words'].value_counts()[1:]\n\nneg_comments = list(df[df['sentiment_negative']==1]['reviews'].values)\nnegative_words = ' '.join(neg_comments).split(' ')\nnegative_words = pd.DataFrame(negative_words,columns=['words'])['words'].value_counts()[1:]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:53.902852Z","iopub.status.busy":"2021-03-08T18:21:53.902078Z","iopub.status.idle":"2021-03-08T18:21:56.293346Z","shell.execute_reply":"2021-03-08T18:21:56.292673Z"},"papermill":{"duration":2.50993,"end_time":"2021-03-08T18:21:56.293569","exception":false,"start_time":"2021-03-08T18:21:53.783639","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"positive_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42,\n                      colormap='Blues', background_color='white', max_words=50)\npositive_wc = positive_wc.generate_from_frequencies(positive_words)\n\n\nneutral_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42,\n                          colormap='Greys', background_color='white', max_words=50)\nneutral_wc = neutral_wc.generate_from_frequencies(neutral_words)\n\n\nnegative_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42,\n                          colormap='Reds_r', background_color='white', max_words=50)\nnegative_wc = negative_wc.generate_from_frequencies(negative_words)\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:56.507505Z","iopub.status.busy":"2021-03-08T18:21:56.506666Z","iopub.status.idle":"2021-03-08T18:21:57.936283Z","shell.execute_reply":"2021-03-08T18:21:57.936849Z"},"papermill":{"duration":1.54043,"end_time":"2021-03-08T18:21:57.937109","exception":false,"start_time":"2021-03-08T18:21:56.396679","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(30, 30))\n\naxs[0].set_title('WordCloud for Positive', size=25, pad=20)\naxs[0].imshow(positive_wc)\naxs[0].axis('off')\n\naxs[1].set_title('WordCloud for Neutral', size=25, pad=20)\naxs[1].imshow(neutral_wc)\naxs[1].axis('off')\n\naxs[2].set_title('WordCloud for Negative', size=25, pad=20)\naxs[2].imshow(negative_wc)\naxs[2].axis('off');\n\nfig.subplots_adjust(hspace=1,wspace=0.0)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.11484,"end_time":"2021-03-08T18:21:58.166479","exception":false,"start_time":"2021-03-08T18:21:58.051639","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We can see the words like Easy, Love, Good in Positive Words Cloud. Order, Go, Get in Neutral and Update, Try and Search in the Negative Word Clouds. However words alone can have a lot of different meanings. To get more context we will use the N_grams to identify what kind of thing they are talking about in each sentiment."},{"metadata":{"papermill":{"duration":0.113121,"end_time":"2021-03-08T18:21:58.393621","exception":false,"start_time":"2021-03-08T18:21:58.2805","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"NGP\"></a>\n## 6.3 - N_Gram Plot"},{"metadata":{"papermill":{"duration":0.113506,"end_time":"2021-03-08T18:21:58.621207","exception":false,"start_time":"2021-03-08T18:21:58.507701","status":"completed"},"tags":[]},"cell_type":"markdown","source":" \n That is a very nice way to understand the most comuns topics per sentiment. To do that we will use the module CountVectorizer from sklearn feature_extraction text library. \n It might seem a little bit of a mess at first look but it's not.\n     We builded a function that counted the 1,2,3 and 4 grams per sentiment and another function to define the dfs per sentiment calling the first function.\n\n Then we will end up with amazon_results df that contain a list of 12 dfs. \n    \n   * Mono-gram - positive neutral and negative.\n   * Bi-gram - for positive neutral and negative.\n   * Tri-gram - for positive neutral and negative.\n   * Tetra-gram - for positive neutral and negative.\n\nThen we will use subplot to plot the top 5 most comuns n-grams for each sentiment.\n\n     "},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:58.858449Z","iopub.status.busy":"2021-03-08T18:21:58.857583Z","iopub.status.idle":"2021-03-08T18:21:58.862368Z","shell.execute_reply":"2021-03-08T18:21:58.862975Z"},"papermill":{"duration":0.126155,"end_time":"2021-03-08T18:21:58.863199","exception":false,"start_time":"2021-03-08T18:21:58.737044","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:59.117696Z","iopub.status.busy":"2021-03-08T18:21:59.116553Z","iopub.status.idle":"2021-03-08T18:21:59.155858Z","shell.execute_reply":"2021-03-08T18:21:59.156809Z"},"papermill":{"duration":0.171581,"end_time":"2021-03-08T18:21:59.157175","exception":false,"start_time":"2021-03-08T18:21:58.985594","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def ngram_results(df):\n    \n    df_1=df[df['sentiment_positive']==1]\n    df_2=df[df['sentiment_neutral']==1]\n    df_3=df[df['sentiment_negative']==1]\n    \n    \n    def uni_bi_tri(df_pos,df_neu,df_neg):\n        \n        #pos\n        word_vectorizer = CountVectorizer(ngram_range=(1,1), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_pos['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_mono_pos=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n\n        word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_pos['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_bi_pos=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n\n        word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_pos['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_tri_pos=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n        \n        word_vectorizer = CountVectorizer(ngram_range=(4,4), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_pos['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_tetra_pos=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n       \n        #neu\n        word_vectorizer = CountVectorizer(ngram_range=(1,1), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_neu['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_mono_neu=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n\n        word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_neu['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_bi_neu=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n\n        word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_neu['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_tri_neu=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n        \n        word_vectorizer = CountVectorizer(ngram_range=(4,4), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_neu['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_tetra_neu=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n        \n        #neg\n        word_vectorizer = CountVectorizer(ngram_range=(1,1), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_neg['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_mono_neg=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n\n        word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_neg['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_bi_neg=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n\n        word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_neg['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_tri_neg=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n        \n        word_vectorizer = CountVectorizer(ngram_range=(4,4), analyzer='word')\n        sparse_matrix = word_vectorizer.fit_transform(df_neg['reviews'][:10000])\n        frequencies = sum(sparse_matrix).toarray()[0]\n        result_tetra_neg=pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=['frequency'],ascending=False)\n        \n        results=[result_mono_pos,result_bi_pos,result_tri_pos,result_tetra_pos,\n                 result_mono_neu,result_bi_neu,result_tri_neu,result_tetra_neu,\n                 result_mono_neg,result_bi_neg,result_tri_neg,result_tetra_neg]\n        \n        return results\n    \n\n    \n    return uni_bi_tri(df_1,df_2,df_3)\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:21:59.409381Z","iopub.status.busy":"2021-03-08T18:21:59.408192Z","iopub.status.idle":"2021-03-08T18:23:55.736518Z","shell.execute_reply":"2021-03-08T18:23:55.737068Z"},"papermill":{"duration":116.457815,"end_time":"2021-03-08T18:23:55.737328","exception":false,"start_time":"2021-03-08T18:21:59.279513","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"amazon_results = ngram_results(df)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:23:55.973737Z","iopub.status.busy":"2021-03-08T18:23:55.972653Z","iopub.status.idle":"2021-03-08T18:23:55.985113Z","shell.execute_reply":"2021-03-08T18:23:55.985633Z"},"papermill":{"duration":0.132003,"end_time":"2021-03-08T18:23:55.985866","exception":false,"start_time":"2021-03-08T18:23:55.853863","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def rename_index(dflist):\n\n    #pos\n    df_mono_pos=pd.DataFrame(dflist[0]).reset_index().rename(columns={'index':'Mono_Pos'})\n    df_bi_pos=pd.DataFrame(dflist[1]).reset_index().rename(columns={'index':'Bi_Pos'})\n    df_tri_pos=pd.DataFrame(dflist[2]).reset_index().rename(columns={'index':'Tri_Pos'})\n    df_tetra_pos=pd.DataFrame(dflist[3]).reset_index().rename(columns={'index':'Tetra_Pos'})\n    #neu\n    df_mono_neu=pd.DataFrame(dflist[4]).reset_index().rename(columns={'index':'Mono_Neu'})\n    df_bi_neu=pd.DataFrame(dflist[5]).reset_index().rename(columns={'index':'Bi_Neu'})\n    df_tri_neu=pd.DataFrame(dflist[6]).reset_index().rename(columns={'index':'Tri_Neu'})\n    df_tetra_neu=pd.DataFrame(dflist[7]).reset_index().rename(columns={'index':'Tetra_Neu'})\n    #neg\n    df_mono_neg=pd.DataFrame(dflist[8]).reset_index().rename(columns={'index':'Mono_Pos'})\n    df_bi_neg=pd.DataFrame(dflist[9]).reset_index().rename(columns={'index':'Bi_Pos'})\n    df_tri_neg=pd.DataFrame(dflist[10]).reset_index().rename(columns={'index':'Tri_Pos'})\n    df_tetra_neg=pd.DataFrame(dflist[11]).reset_index().rename(columns={'index':'Tetra_Neg'})\n    \n    result=[df_mono_pos, df_bi_pos, df_tri_pos, df_tetra_pos,\n            df_mono_neu, df_bi_neu, df_tri_neu, df_tetra_neu,\n            df_mono_neg, df_bi_neg, df_tri_neg, df_tetra_neg]\n    \n    return result","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:23:56.219064Z","iopub.status.busy":"2021-03-08T18:23:56.217962Z","iopub.status.idle":"2021-03-08T18:23:56.316976Z","shell.execute_reply":"2021-03-08T18:23:56.317488Z"},"papermill":{"duration":0.217081,"end_time":"2021-03-08T18:23:56.317717","exception":false,"start_time":"2021-03-08T18:23:56.100636","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"amazon_results=rename_index(amazon_results)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:23:56.550307Z","iopub.status.busy":"2021-03-08T18:23:56.549215Z","iopub.status.idle":"2021-03-08T18:23:56.805169Z","shell.execute_reply":"2021-03-08T18:23:56.804555Z"},"papermill":{"duration":0.373567,"end_time":"2021-03-08T18:23:56.805342","exception":false,"start_time":"2021-03-08T18:23:56.431775","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"#for df in amazon_results:\nfig = make_subplots(rows=3, cols=4,\n                    vertical_spacing=0.15,\n                    column_titles =['Mono-grams','Bi-grams','Tri-grams','Tetra-grams'],\n                    row_titles =['Positive','Neutral','Negative'],                  \n      \n                   )\n#for df in amazon_results:\nfig.add_trace(go.Bar(\n            x = amazon_results[0][amazon_results[0].columns[0]].head(),\n            y = amazon_results[0][amazon_results[0].columns[1]].head(),\n            marker_color='DeepSkyBlue',\n            ),row=1, col=1)\nfig.add_trace(go.Bar(\n            x = amazon_results[1][amazon_results[1].columns[0]].head(),\n            y = amazon_results[1][amazon_results[1].columns[1]].head(),\n            marker_color='DeepSkyBlue',\n            ),row=1, col=2)\nfig.add_trace(go.Bar(\n            x = amazon_results[2][amazon_results[2].columns[0]].head(),\n            y = amazon_results[2][amazon_results[2].columns[1]].head(),\n            marker_color='DeepSkyBlue',\n            ),row=1, col=3)\n    # neu\nfig.add_trace(go.Bar(\n            x = amazon_results[3][amazon_results[3].columns[0]].head(),\n            y = amazon_results[3][amazon_results[3].columns[1]].head(),\n            marker_color='DeepSkyBlue',\n            ),row=1, col=4)\nfig.add_trace(go.Bar(\n            x = amazon_results[4][amazon_results[4].columns[0]].head(),\n            y = amazon_results[4][amazon_results[4].columns[1]].head(),\n            marker_color='Lightgrey',\n            ),row=2, col=1)\nfig.add_trace(go.Bar(\n            x = amazon_results[5][amazon_results[5].columns[0]].head(),\n            y = amazon_results[5][amazon_results[5].columns[1]].head(),\n            marker_color='Lightgrey',\n            ),row=2, col=2)\n    # neg\nfig.add_trace(go.Bar(\n            x = amazon_results[6][amazon_results[6].columns[0]].head(),\n            y = amazon_results[6][amazon_results[6].columns[1]].head(),\n            marker_color='Lightgrey',\n            ),row=2, col=3)\nfig.add_trace(go.Bar(\n            x = amazon_results[7][amazon_results[7].columns[0]].head(),\n            y = amazon_results[7][amazon_results[7].columns[1]].head(),\n            marker_color='Lightgrey',\n            ),row=2, col=4)\nfig.add_trace(go.Bar(\n            x = amazon_results[8][amazon_results[8].columns[0]].head(),\n            y = amazon_results[8][amazon_results[8].columns[1]].head(),\n            marker_color='Crimson',\n            ),row=3, col=1)\nfig.add_trace(go.Bar(\n            x = amazon_results[9][amazon_results[9].columns[0]].head(),\n            y = amazon_results[9][amazon_results[9].columns[1]].head(),\n            marker_color='Crimson',\n            ),row=3, col=2)\nfig.add_trace(go.Bar(\n            x = amazon_results[10][amazon_results[10].columns[0]].head(),\n            y = amazon_results[10][amazon_results[10].columns[1]].head(),\n            marker_color='Crimson',\n            ),row=3, col=3)\nfig.add_trace(go.Bar(\n            x = amazon_results[11][amazon_results[11].columns[0]].head(),\n            y = amazon_results[11][amazon_results[11].columns[1]].head(),\n            marker_color='Crimson',\n            ),row=3, col=4)\n\n\n    \n#\nfig.update_layout({\"showlegend\": False},height=900, width=1000)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.11639,"end_time":"2021-03-08T18:23:57.03957","exception":false,"start_time":"2021-03-08T18:23:56.92318","status":"completed"},"tags":[]},"cell_type":"markdown","source":"For positive reviews we usually see easy use, love shop amazon or great customer service. For neutral reviews some specific topics that maybe people are facing troubles and it should be improved like wish list, kindle book, amazon smile or something to bring back. Negative reviews usually see people talking about the latest updates and we see some error messages like \"something went wrong\" among more other things. \n \nIt makes way more sense isn't it? \n \nWhen smaller is the n_gram shows more similarity between the sentiments and the opposite happens when we increase the number of grams.\n"},{"metadata":{"papermill":{"duration":0.115487,"end_time":"2021-03-08T18:23:57.2699","exception":false,"start_time":"2021-03-08T18:23:57.154413","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"Modeling\"></a>\n# 7 - Modeling"},{"metadata":{"papermill":{"duration":0.114174,"end_time":"2021-03-08T18:23:57.498596","exception":false,"start_time":"2021-03-08T18:23:57.384422","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Before we build our deep learning model, we are going to go through a few steps. The first one is to Split the data frame into Train and Test. Second, we will vectorize and embed the reviews. \nIn order to prevent overfitting we will balance the train set and also reduce the amount of features of it. Then we are going to create 3  functions of metrics that will be used into our model.\n"},{"metadata":{"papermill":{"duration":0.114847,"end_time":"2021-03-08T18:23:57.728535","exception":false,"start_time":"2021-03-08T18:23:57.613688","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"TTS\"></a>\n## 7.1 - Train Test Split"},{"metadata":{"papermill":{"duration":0.115453,"end_time":"2021-03-08T18:23:57.9605","exception":false,"start_time":"2021-03-08T18:23:57.845047","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now, we will divide our data into training and test sets using sklearn.model_selection module train_test_split."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:23:58.19755Z","iopub.status.busy":"2021-03-08T18:23:58.196813Z","iopub.status.idle":"2021-03-08T18:23:58.199597Z","shell.execute_reply":"2021-03-08T18:23:58.199037Z"},"papermill":{"duration":0.122938,"end_time":"2021-03-08T18:23:58.199765","exception":false,"start_time":"2021-03-08T18:23:58.076827","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:23:58.439501Z","iopub.status.busy":"2021-03-08T18:23:58.438771Z","iopub.status.idle":"2021-03-08T18:23:58.441725Z","shell.execute_reply":"2021-03-08T18:23:58.442366Z"},"papermill":{"duration":0.126806,"end_time":"2021-03-08T18:23:58.442599","exception":false,"start_time":"2021-03-08T18:23:58.315793","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"y =  df[df.columns[1:]].values","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:23:58.677661Z","iopub.status.busy":"2021-03-08T18:23:58.676946Z","iopub.status.idle":"2021-03-08T18:23:58.690381Z","shell.execute_reply":"2021-03-08T18:23:58.690889Z"},"papermill":{"duration":0.133452,"end_time":"2021-03-08T18:23:58.69118","exception":false,"start_time":"2021-03-08T18:23:58.557728","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"X = []\n\nsentences = list(df[\"reviews\"])\nfor sen in sentences:\n    X.append(sen)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:23:58.928134Z","iopub.status.busy":"2021-03-08T18:23:58.927433Z","iopub.status.idle":"2021-03-08T18:23:58.945021Z","shell.execute_reply":"2021-03-08T18:23:58.944364Z"},"papermill":{"duration":0.138852,"end_time":"2021-03-08T18:23:58.945193","exception":false,"start_time":"2021-03-08T18:23:58.806341","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.114936,"end_time":"2021-03-08T18:23:59.175599","exception":false,"start_time":"2021-03-08T18:23:59.060663","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"E\"></a>\n## 7.2 - Embedding"},{"metadata":{"papermill":{"duration":0.114781,"end_time":"2021-03-08T18:23:59.40615","exception":false,"start_time":"2021-03-08T18:23:59.291369","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We need to convert text inputs into embedded vectors such that we can apply deep learning. In word embeddings, every word is represented as an n-dimensional dense vector. The words that are similar will have similar vectors. [See more about embedding](https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/)\n \n In order to embed our text, first we need to transform our reviews into vector representations. Here we will use Tokenizer module from keras.preprocessing.text to vectorize the text corpus and we will need the module pad_sequences  from keras.preprocessing.sequence to ensure that all sequences in the list of words have the same length. \n \nAfter vectorising the text and ensuring that all sequences have the same length we will use the GloVe, Global Vectors For Word Representation to convert text inputs to their numeric counterparts. [See more about GloVe](https://nlp.stanford.edu/projects/glove/)\n\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:23:59.648371Z","iopub.status.busy":"2021-03-08T18:23:59.647472Z","iopub.status.idle":"2021-03-08T18:24:06.749294Z","shell.execute_reply":"2021-03-08T18:24:06.749989Z"},"papermill":{"duration":7.226554,"end_time":"2021-03-08T18:24:06.750219","exception":false,"start_time":"2021-03-08T18:23:59.523665","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:06.989552Z","iopub.status.busy":"2021-03-08T18:24:06.98878Z","iopub.status.idle":"2021-03-08T18:24:08.004584Z","shell.execute_reply":"2021-03-08T18:24:08.005196Z"},"papermill":{"duration":1.136392,"end_time":"2021-03-08T18:24:08.005442","exception":false,"start_time":"2021-03-08T18:24:06.86905","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Defining the num of words\ntokenizer = Tokenizer(num_words=5000)\n# Fighting in the X df\ntokenizer.fit_on_texts(X)\n\n# Tokenizing Train and test\n\nX_prep = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nvocab_size = len(tokenizer.word_index) + 1\n\n# Setting the max length of features\n\nmaxlen = 200\n\n# Ensuring the all sequences have the same length\nX_train = pad_sequences(X_prep, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.115893,"end_time":"2021-03-08T18:24:08.240431","exception":false,"start_time":"2021-03-08T18:24:08.124538","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We will be using numpy arrays and GloVe word embeddings to convert text inputs to their numeric counterparts."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:08.476602Z","iopub.status.busy":"2021-03-08T18:24:08.475835Z","iopub.status.idle":"2021-03-08T18:24:08.480773Z","shell.execute_reply":"2021-03-08T18:24:08.480232Z"},"papermill":{"duration":0.124711,"end_time":"2021-03-08T18:24:08.480972","exception":false,"start_time":"2021-03-08T18:24:08.356261","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:08.722803Z","iopub.status.busy":"2021-03-08T18:24:08.722021Z","iopub.status.idle":"2021-03-08T18:24:28.736084Z","shell.execute_reply":"2021-03-08T18:24:28.734799Z"},"papermill":{"duration":20.138782,"end_time":"2021-03-08T18:24:28.736278","exception":false,"start_time":"2021-03-08T18:24:08.597496","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"embeddings_dictionary = dict()\n\nglove_file = open('../input/glove6b100dtxt/glove.6B.100d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()\n\nembedding_matrix = zeros((vocab_size, 100))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.119294,"end_time":"2021-03-08T18:24:28.973142","exception":false,"start_time":"2021-03-08T18:24:28.853848","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"B\"></a>\n## 7.3 - Balancing"},{"metadata":{"papermill":{"duration":0.116701,"end_time":"2021-03-08T18:24:29.207457","exception":false,"start_time":"2021-03-08T18:24:29.090756","status":"completed"},"tags":[]},"cell_type":"markdown","source":"As seen before, the data set is highly unbalanced, to be more precise in the results we will balance the dataset using the imblearn Random OverSampling technique."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:29.44882Z","iopub.status.busy":"2021-03-08T18:24:29.448059Z","iopub.status.idle":"2021-03-08T18:24:29.462948Z","shell.execute_reply":"2021-03-08T18:24:29.4637Z"},"papermill":{"duration":0.140212,"end_time":"2021-03-08T18:24:29.463971","exception":false,"start_time":"2021-03-08T18:24:29.323759","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"print('Negative',pd.DataFrame(y_train).sum()[0])\nprint('Neutral',pd.DataFrame(y_train).sum()[1])\nprint('Positve',pd.DataFrame(y_train).sum()[2])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:29.703081Z","iopub.status.busy":"2021-03-08T18:24:29.702003Z","iopub.status.idle":"2021-03-08T18:24:30.070566Z","shell.execute_reply":"2021-03-08T18:24:30.069572Z"},"papermill":{"duration":0.489331,"end_time":"2021-03-08T18:24:30.070775","exception":false,"start_time":"2021-03-08T18:24:29.581444","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:30.31428Z","iopub.status.busy":"2021-03-08T18:24:30.313449Z","iopub.status.idle":"2021-03-08T18:24:30.360489Z","shell.execute_reply":"2021-03-08T18:24:30.361285Z"},"papermill":{"duration":0.17139,"end_time":"2021-03-08T18:24:30.361572","exception":false,"start_time":"2021-03-08T18:24:30.190182","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"over = RandomOverSampler()\n    \nX_ov, y_ov = over.fit_resample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:30.605985Z","iopub.status.busy":"2021-03-08T18:24:30.604849Z","iopub.status.idle":"2021-03-08T18:24:30.618153Z","shell.execute_reply":"2021-03-08T18:24:30.618787Z"},"papermill":{"duration":0.136353,"end_time":"2021-03-08T18:24:30.619025","exception":false,"start_time":"2021-03-08T18:24:30.482672","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"print('Negative',pd.DataFrame(y_ov).sum()[0])\nprint('Neutral',pd.DataFrame(y_ov).sum()[1])\nprint('Positve',pd.DataFrame(y_ov).sum()[2])\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.117945,"end_time":"2021-03-08T18:24:30.855795","exception":false,"start_time":"2021-03-08T18:24:30.73785","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"FS\"></a>\n## 7.4 - Featuring Selection"},{"metadata":{"papermill":{"duration":0.121627,"end_time":"2021-03-08T18:24:31.098677","exception":false,"start_time":"2021-03-08T18:24:30.97705","status":"completed"},"tags":[]},"cell_type":"markdown","source":"In order to prevent overfitting we are going to reduce the features of the data frame. To do that we are going to use one of the simplest and most common ways to select relevant features for classification which is to calculate the F-Score for each feature. \n \nThe F-Score is calculated using the variance between the features and the variance within each feature. A small F-score usually means that the feature is less important than a feature with a high F-score. We will calculate the F-Score of the features per sentiment using sklearn modules SelectKBest and f_classif to return the ANOVA F-value.\n \n \nI highly recommend see the Nils Schl√ºter [Medium article](https://towardsdatascience.com/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323) for more ways to prevent overfitting in Deep Learning Models.\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:31.347205Z","iopub.status.busy":"2021-03-08T18:24:31.346304Z","iopub.status.idle":"2021-03-08T18:24:31.365947Z","shell.execute_reply":"2021-03-08T18:24:31.365247Z"},"papermill":{"duration":0.144177,"end_time":"2021-03-08T18:24:31.366154","exception":false,"start_time":"2021-03-08T18:24:31.221977","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import f_classif, SelectKBest","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:31.614095Z","iopub.status.busy":"2021-03-08T18:24:31.612552Z","iopub.status.idle":"2021-03-08T18:24:31.716781Z","shell.execute_reply":"2021-03-08T18:24:31.715887Z"},"papermill":{"duration":0.230327,"end_time":"2021-03-08T18:24:31.71698","exception":false,"start_time":"2021-03-08T18:24:31.486653","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"selected_features = [] \nfor label in range(0,3):\n    selector = SelectKBest(f_classif, k='all')\n    selector.fit(X_ov, pd.DataFrame(y_ov)[label])\n    selected_features.append(list(selector.scores_))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:31.979169Z","iopub.status.busy":"2021-03-08T18:24:31.968418Z","iopub.status.idle":"2021-03-08T18:24:32.188942Z","shell.execute_reply":"2021-03-08T18:24:32.188302Z"},"papermill":{"duration":0.350042,"end_time":"2021-03-08T18:24:32.18911","exception":false,"start_time":"2021-03-08T18:24:31.839068","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nplt.plot(selected_features[0])\nplt.plot(selected_features[1])\nplt.plot(selected_features[2])\n\nplt.title('ANOVA F-value')\nplt.ylabel('F-value')\nplt.xlabel('n_feature')\nplt.legend(['negative','neutral','positive'], loc='upper right')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.122645,"end_time":"2021-03-08T18:24:32.433451","exception":false,"start_time":"2021-03-08T18:24:32.310806","status":"completed"},"tags":[]},"cell_type":"markdown","source":" As seeing, usually the reviews have the highest F-Scores till the feature around 10. After that the F_store is visually decreasing. \nSo, we will arbitrarily drop the features after the 20.\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:32.699258Z","iopub.status.busy":"2021-03-08T18:24:32.696553Z","iopub.status.idle":"2021-03-08T18:24:32.941094Z","shell.execute_reply":"2021-03-08T18:24:32.941634Z"},"papermill":{"duration":0.387597,"end_time":"2021-03-08T18:24:32.941841","exception":false,"start_time":"2021-03-08T18:24:32.554244","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nplt.plot(pd.DataFrame(selected_features).T[:20][0])\nplt.plot(pd.DataFrame(selected_features).T[:20][1])\nplt.plot(pd.DataFrame(selected_features).T[:20][2])\n\nplt.title('ANOVA F-value')\nplt.ylabel('F-value')\nplt.xlabel('n_feature')\nplt.legend(['negative','neutral','positive'], loc='upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:33.192437Z","iopub.status.busy":"2021-03-08T18:24:33.19165Z","iopub.status.idle":"2021-03-08T18:24:33.195068Z","shell.execute_reply":"2021-03-08T18:24:33.195581Z"},"papermill":{"duration":0.131492,"end_time":"2021-03-08T18:24:33.195808","exception":false,"start_time":"2021-03-08T18:24:33.064316","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"X_ov=X_ov[:,:20]","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.122964,"end_time":"2021-03-08T18:24:33.442342","exception":false,"start_time":"2021-03-08T18:24:33.319378","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"EM\"></a>\n## 7.5 -  Evaluation Metrics"},{"metadata":{"papermill":{"duration":0.121661,"end_time":"2021-03-08T18:24:33.686132","exception":false,"start_time":"2021-03-08T18:24:33.564471","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To go beyond just the Accuracy metric we will calculate also the  Recall, Precision and F1 Score manually.\n\nReviewing accuracy, recall, precision and f1 score. We are using the [Exsilio Solutions](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/#:~:text=F1%20score%20%2D%20F1%20Score%20is,have%20an%20uneven%20class%20distribution) explanations.\n\n   * TP - True Positive\n   * TN - True Negative\n   * FP - False Positive\n   * FN - False Negative\n\n**Accuracy** -  \"Is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same\" \n \n   * Accuracy = TP+TN/TP+FP+FN+TN\n\n\n**Precision** - \"Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\"\n\n   * Precision = TP/TP+FP\n\n**Recall** - \"Recall is the ratio of correctly predicted positive observations to the all observations in actual class\"\n   * Recall = TP/TP+FN\n\n**F1 score** - \"F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution.\"\n\n   * F1 Score  = 2*(Recall * Precision) / (Recall + Precision)\n \nThose metrics have been removed from Keras core. [See the release notes](https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes). \n \nSo to do that we will use the three following functions that will be called by the Neural Networks.\nThat functions are made by [Tasos](https://datascience.stackexchange.com/users/201/tasos) answering a question at [Stack Exchange](https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model)\n\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:33.940054Z","iopub.status.busy":"2021-03-08T18:24:33.938844Z","iopub.status.idle":"2021-03-08T18:24:33.942196Z","shell.execute_reply":"2021-03-08T18:24:33.941354Z"},"papermill":{"duration":0.134327,"end_time":"2021-03-08T18:24:33.942395","exception":false,"start_time":"2021-03-08T18:24:33.808068","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from tensorflow.keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:34.1998Z","iopub.status.busy":"2021-03-08T18:24:34.199053Z","iopub.status.idle":"2021-03-08T18:24:34.201594Z","shell.execute_reply":"2021-03-08T18:24:34.200952Z"},"papermill":{"duration":0.132539,"end_time":"2021-03-08T18:24:34.201766","exception":false,"start_time":"2021-03-08T18:24:34.069227","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:34.454633Z","iopub.status.busy":"2021-03-08T18:24:34.453782Z","iopub.status.idle":"2021-03-08T18:24:34.457316Z","shell.execute_reply":"2021-03-08T18:24:34.456605Z"},"papermill":{"duration":0.133291,"end_time":"2021-03-08T18:24:34.457506","exception":false,"start_time":"2021-03-08T18:24:34.324215","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:34.714387Z","iopub.status.busy":"2021-03-08T18:24:34.713619Z","iopub.status.idle":"2021-03-08T18:24:34.715759Z","shell.execute_reply":"2021-03-08T18:24:34.716434Z"},"papermill":{"duration":0.13389,"end_time":"2021-03-08T18:24:34.716644","exception":false,"start_time":"2021-03-08T18:24:34.582754","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.127511,"end_time":"2021-03-08T18:24:34.96861","exception":false,"start_time":"2021-03-08T18:24:34.841099","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"NN\"></a>\n## 7.6 -  Neural Networks"},{"metadata":{"papermill":{"duration":0.123335,"end_time":"2021-03-08T18:24:35.219062","exception":false,"start_time":"2021-03-08T18:24:35.095727","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To create a model to predict the text sentiment, we will have one input layer shape 20, the number of features. One Embedding Layer that takes the vocab size and the embedding matrix created before. One Long Short-Term Memory (LSTM) layer with 128 neurons and finely the output layer with 3 neurons since we have 3 labels in the output, Positive, Neutral and Negative.\n\nWe are going to use keras model module and keras.layers, Dense, LSTM and Embedding.\n\nFor the metrics we will set Accuracy, F1, Precision and Recall that we defined above.\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:35.47268Z","iopub.status.busy":"2021-03-08T18:24:35.471756Z","iopub.status.idle":"2021-03-08T18:24:35.478548Z","shell.execute_reply":"2021-03-08T18:24:35.477832Z"},"papermill":{"duration":0.135043,"end_time":"2021-03-08T18:24:35.478733","exception":false,"start_time":"2021-03-08T18:24:35.34369","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, LSTM, Input, Embedding","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:35.730516Z","iopub.status.busy":"2021-03-08T18:24:35.729755Z","iopub.status.idle":"2021-03-08T18:24:36.637582Z","shell.execute_reply":"2021-03-08T18:24:36.636898Z"},"papermill":{"duration":1.034385,"end_time":"2021-03-08T18:24:36.637777","exception":false,"start_time":"2021-03-08T18:24:35.603392","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"deep_inputs = Input(shape=(20,))\nembedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\nLSTM_Layer_1 = LSTM(128)(embedding_layer)\ndense_layer_1 = Dense(3, activation='sigmoid')(LSTM_Layer_1)\nmodel = Model(inputs=deep_inputs, outputs=dense_layer_1)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.127647,"end_time":"2021-03-08T18:24:36.894007","exception":false,"start_time":"2021-03-08T18:24:36.76636","status":"completed"},"tags":[]},"cell_type":"markdown","source":"\nPrinting the model summary:\n"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:37.155567Z","iopub.status.busy":"2021-03-08T18:24:37.154465Z","iopub.status.idle":"2021-03-08T18:24:37.161988Z","shell.execute_reply":"2021-03-08T18:24:37.161284Z"},"papermill":{"duration":0.138632,"end_time":"2021-03-08T18:24:37.162158","exception":false,"start_time":"2021-03-08T18:24:37.023526","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.124037,"end_time":"2021-03-08T18:24:37.409883","exception":false,"start_time":"2021-03-08T18:24:37.285846","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To visualize, we will printing the architecture of our neural network with the plot_model from keras utils"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:37.665225Z","iopub.status.busy":"2021-03-08T18:24:37.664164Z","iopub.status.idle":"2021-03-08T18:24:38.374779Z","shell.execute_reply":"2021-03-08T18:24:38.374034Z"},"papermill":{"duration":0.841362,"end_time":"2021-03-08T18:24:38.374989","exception":false,"start_time":"2021-03-08T18:24:37.533627","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.126983,"end_time":"2021-03-08T18:24:38.627417","exception":false,"start_time":"2021-03-08T18:24:38.500434","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Let's train our model with 0.7 of the random oversampling DF. Batch_size 32 and apochs 10. The other 0.3 of the DF will be used for the validation."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:24:38.889902Z","iopub.status.busy":"2021-03-08T18:24:38.889064Z","iopub.status.idle":"2021-03-08T18:28:03.123336Z","shell.execute_reply":"2021-03-08T18:28:03.122671Z"},"papermill":{"duration":204.370284,"end_time":"2021-03-08T18:28:03.123535","exception":false,"start_time":"2021-03-08T18:24:38.753251","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"history = model.fit(X_ov, y_ov, batch_size=32, epochs=15, verbose=1, validation_split=0.3)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.993709,"end_time":"2021-03-08T18:28:05.10613","exception":false,"start_time":"2021-03-08T18:28:04.112421","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"Results\"></a>\n# 8 - Results"},{"metadata":{"papermill":{"duration":0.988185,"end_time":"2021-03-08T18:28:07.121085","exception":false,"start_time":"2021-03-08T18:28:06.1329","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now let's plot the results that our model got. First let's plot the history of the metrics that the model got during the 10 epochs in the train and in the validation data set. \nThen we are going to build a table to see the metrics \nachieved in the train and in the test data set and finely we will build a function that put our model in context getting a human comment, appling the text preprocessing and the model to plot the label predicted by our model.\n"},{"metadata":{"papermill":{"duration":0.983987,"end_time":"2021-03-08T18:28:09.157212","exception":false,"start_time":"2021-03-08T18:28:08.173225","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"PH\"></a>\n## 8.1 -  Plotting History"},{"metadata":{"papermill":{"duration":0.986503,"end_time":"2021-03-08T18:28:11.129917","exception":false,"start_time":"2021-03-08T18:28:10.143414","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now, we are going to use matplotlib again to plot the model History that recorded training and validation metrics for each epoch. The results obtained in history for Loss, Accuracy, Precision, Recall and F1 score for Training in blue and Validation in orange."},{"metadata":{"papermill":{"duration":0.9835,"end_time":"2021-03-08T18:28:13.100537","exception":false,"start_time":"2021-03-08T18:28:12.117037","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Loss"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:28:15.140158Z","iopub.status.busy":"2021-03-08T18:28:15.139454Z","iopub.status.idle":"2021-03-08T18:28:15.335757Z","shell.execute_reply":"2021-03-08T18:28:15.336379Z"},"papermill":{"duration":1.25192,"end_time":"2021-03-08T18:28:15.336593","exception":false,"start_time":"2021-03-08T18:28:14.084673","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','val'], loc='upper left')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.01547,"end_time":"2021-03-08T18:28:17.339889","exception":false,"start_time":"2021-03-08T18:28:16.324419","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Accuracy"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:28:19.328493Z","iopub.status.busy":"2021-03-08T18:28:19.327717Z","iopub.status.idle":"2021-03-08T18:28:19.526399Z","shell.execute_reply":"2021-03-08T18:28:19.527135Z"},"papermill":{"duration":1.194685,"end_time":"2021-03-08T18:28:19.527349","exception":false,"start_time":"2021-03-08T18:28:18.332664","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','val'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.99421,"end_time":"2021-03-08T18:28:21.51319","exception":false,"start_time":"2021-03-08T18:28:20.51898","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Precision"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:28:23.562303Z","iopub.status.busy":"2021-03-08T18:28:23.561553Z","iopub.status.idle":"2021-03-08T18:28:23.753378Z","shell.execute_reply":"2021-03-08T18:28:23.752722Z"},"papermill":{"duration":1.251971,"end_time":"2021-03-08T18:28:23.753542","exception":false,"start_time":"2021-03-08T18:28:22.501571","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.plot(history.history['precision_m'])\nplt.plot(history.history['val_precision_m'])\n\nplt.title('model precision')\nplt.ylabel('precision')\nplt.xlabel('epoch')\nplt.legend(['train','val'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.992475,"end_time":"2021-03-08T18:28:25.732344","exception":false,"start_time":"2021-03-08T18:28:24.739869","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Recall"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:28:27.888104Z","iopub.status.busy":"2021-03-08T18:28:27.887347Z","iopub.status.idle":"2021-03-08T18:28:28.030513Z","shell.execute_reply":"2021-03-08T18:28:28.029749Z"},"papermill":{"duration":1.260739,"end_time":"2021-03-08T18:28:28.030717","exception":false,"start_time":"2021-03-08T18:28:26.769978","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.plot(history.history['recall_m'])\nplt.plot(history.history['val_recall_m'])\n\nplt.title('model recall')\nplt.ylabel('recall')\nplt.xlabel('epoch')\nplt.legend(['train','val'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.988098,"end_time":"2021-03-08T18:28:30.007871","exception":false,"start_time":"2021-03-08T18:28:29.019773","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### F1 Score"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:28:32.102735Z","iopub.status.busy":"2021-03-08T18:28:32.089066Z","iopub.status.idle":"2021-03-08T18:28:32.261694Z","shell.execute_reply":"2021-03-08T18:28:32.261155Z"},"papermill":{"duration":1.263139,"end_time":"2021-03-08T18:28:32.261872","exception":false,"start_time":"2021-03-08T18:28:30.998733","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.plot(history.history['f1_m'])\nplt.plot(history.history['val_f1_m'])\n\nplt.title('model F1')\nplt.ylabel('f1')\nplt.xlabel('epoch')\nplt.legend(['train','val'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.991111,"end_time":"2021-03-08T18:28:34.250664","exception":false,"start_time":"2021-03-08T18:28:33.259553","status":"completed"},"tags":[]},"cell_type":"markdown","source":"It seems like a very good model. The training and validation shows good results and are both always close to each other. Also the model keeps improving each epoch."},{"metadata":{"papermill":{"duration":0.991861,"end_time":"2021-03-08T18:28:36.231896","exception":false,"start_time":"2021-03-08T18:28:35.240035","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"TE\"></a>\n## 8.2 - Test Evaluate"},{"metadata":{"papermill":{"duration":1.062705,"end_time":"2021-03-08T18:28:38.280527","exception":false,"start_time":"2021-03-08T18:28:37.217822","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now let's evaluate in the training and in testing dfs."},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:28:40.311877Z","iopub.status.busy":"2021-03-08T18:28:40.310858Z","iopub.status.idle":"2021-03-08T18:28:49.129288Z","shell.execute_reply":"2021-03-08T18:28:49.128661Z"},"papermill":{"duration":9.818305,"end_time":"2021-03-08T18:28:49.129465","exception":false,"start_time":"2021-03-08T18:28:39.31116","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"score_train = model.evaluate(X_ov[:,:20], y_ov, verbose=1)\nscore_test = model.evaluate(X_test[:,:20], y_test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:28:51.254391Z","iopub.status.busy":"2021-03-08T18:28:51.25357Z","iopub.status.idle":"2021-03-08T18:28:51.256584Z","shell.execute_reply":"2021-03-08T18:28:51.2571Z"},"papermill":{"duration":1.059644,"end_time":"2021-03-08T18:28:51.257294","exception":false,"start_time":"2021-03-08T18:28:50.19765","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"score = pd.DataFrame({'Loss':[score_train[0],score_test[0]],\n             'Accuracy':[score_train[1],score_test[1]],\n             'Precision':[score_train[3],score_test[3]],\n             'Recall':[score_train[4],score_test[4]],\n             'F1-Score':[score_train[2],score_test[2]]\n                     },\n             index=['Train Score','Test Score'])\n\nscore","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.039107,"end_time":"2021-03-08T18:28:53.341168","exception":false,"start_time":"2021-03-08T18:28:52.302061","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Low Loss but high in the test, good Accuracy, Precision, Recall and F1-score for both data frames. Excellent! :)"},{"metadata":{"papermill":{"duration":1.140613,"end_time":"2021-03-08T18:28:55.552494","exception":false,"start_time":"2021-03-08T18:28:54.411881","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"TF\"></a>\n## 8.3 - Test Function"},{"metadata":{"papermill":{"duration":1.044581,"end_time":"2021-03-08T18:28:57.664897","exception":false,"start_time":"2021-03-08T18:28:56.620316","status":"completed"},"tags":[]},"cell_type":"markdown","source":"For the final test we will build a function that puts everything that we learned into context.\n The following function will apply our pipeline for text preprocessing, classify a comment with our model and will return if the comment  is Positive, Negative or Neutral.\n\nLet's see if the model is really accurate in real world comments. üòè"},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:28:59.823768Z","iopub.status.busy":"2021-03-08T18:28:59.820029Z","iopub.status.idle":"2021-03-08T18:28:59.827668Z","shell.execute_reply":"2021-03-08T18:28:59.826953Z"},"papermill":{"duration":1.128447,"end_time":"2021-03-08T18:28:59.82784","exception":false,"start_time":"2021-03-08T18:28:58.699393","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def sentiment_analysis(text, pipeline, model):\n    \n    # Applying the pipeline\n    if type(text) is not list:\n        text = [text]\n        text_prep = text_prep_pipeline.fit_transform(text)\n        text_prep = tokenizer.texts_to_sequences(text_prep)\n        matrix = pad_sequences(text_prep, padding='post', maxlen=20)\n        # Predicting sentiment\n        proba = model.predict(matrix)\n    \n    \n    # Plotting the sentiment and its score\n    fig, ax = plt.subplots(figsize=(5, 3))\n    \n    if proba.argmax() == 0 :\n        text = 'Negative'\n        class_proba = int(proba[0][0].round(2)*100)\n        color = 'Crimson'\n        \n    elif proba.argmax() == 1 :\n        text = 'Neutral'\n        class_proba = int(proba[0][1].round(2)*100) \n        color = 'Lightgrey'\n       \n    elif proba.argmax() == 2 :\n        text = 'Positive'\n        class_proba = int(proba[0][2].round(2)*100) \n        color = 'DeepSkyBlue'\n        \n    ax.text(0.5, 0.5, text, fontsize=50, ha='center', color=color)\n    ax.text(0.5, 0.20, str(class_proba) + '%', fontsize=14, ha='center')\n    ax.axis('off')\n    ax.set_title('Sentiment Analysis', fontsize=14)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:29:01.955395Z","iopub.status.busy":"2021-03-08T18:29:01.954663Z","iopub.status.idle":"2021-03-08T18:29:01.95726Z","shell.execute_reply":"2021-03-08T18:29:01.956586Z"},"papermill":{"duration":1.047717,"end_time":"2021-03-08T18:29:01.957422","exception":false,"start_time":"2021-03-08T18:29:00.909705","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"#Positive\n\ncomment = \"The prices are amazing and the shipping is really fast! Thank for everything amazon!\"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:29:04.112135Z","iopub.status.busy":"2021-03-08T18:29:04.107781Z","iopub.status.idle":"2021-03-08T18:29:04.604752Z","shell.execute_reply":"2021-03-08T18:29:04.604179Z"},"papermill":{"duration":1.614219,"end_time":"2021-03-08T18:29:04.604912","exception":false,"start_time":"2021-03-08T18:29:02.990693","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"sentiment_analysis(comment, pipeline=text_prep_pipeline,  model=model)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:29:06.687978Z","iopub.status.busy":"2021-03-08T18:29:06.687104Z","iopub.status.idle":"2021-03-08T18:29:06.690516Z","shell.execute_reply":"2021-03-08T18:29:06.689694Z"},"papermill":{"duration":1.046072,"end_time":"2021-03-08T18:29:06.690688","exception":false,"start_time":"2021-03-08T18:29:05.644616","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Negative\n\ncomment = \"I lost my money. I contact the custom service and they tell to wait receive item but not received yet\"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:29:08.781866Z","iopub.status.busy":"2021-03-08T18:29:08.781181Z","iopub.status.idle":"2021-03-08T18:29:08.889464Z","shell.execute_reply":"2021-03-08T18:29:08.888263Z"},"papermill":{"duration":1.151691,"end_time":"2021-03-08T18:29:08.889654","exception":false,"start_time":"2021-03-08T18:29:07.737963","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"sentiment_analysis(comment, pipeline=text_prep_pipeline, model=model)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:29:11.049299Z","iopub.status.busy":"2021-03-08T18:29:11.048272Z","iopub.status.idle":"2021-03-08T18:29:11.051003Z","shell.execute_reply":"2021-03-08T18:29:11.051485Z"},"papermill":{"duration":1.114623,"end_time":"2021-03-08T18:29:11.051688","exception":false,"start_time":"2021-03-08T18:29:09.937065","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Neutral\ncomment = \"Overall app is ok. But i really want some improvements in the next update.\"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-03-08T18:29:13.185242Z","iopub.status.busy":"2021-03-08T18:29:13.182972Z","iopub.status.idle":"2021-03-08T18:29:13.289063Z","shell.execute_reply":"2021-03-08T18:29:13.287949Z"},"papermill":{"duration":1.158105,"end_time":"2021-03-08T18:29:13.289253","exception":false,"start_time":"2021-03-08T18:29:12.131148","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"sentiment_analysis(text=comment, pipeline=text_prep_pipeline, model=model)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.045193,"end_time":"2021-03-08T18:29:15.380037","exception":false,"start_time":"2021-03-08T18:29:14.334844","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Yes! :')"},{"metadata":{"papermill":{"duration":1.055911,"end_time":"2021-03-08T18:29:17.48229","exception":false,"start_time":"2021-03-08T18:29:16.426379","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a id=\"CON\"></a>\n# 9 - Conclusion "},{"metadata":{"papermill":{"duration":1.038479,"end_time":"2021-03-08T18:29:19.571663","exception":false,"start_time":"2021-03-08T18:29:18.533184","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We just built a very good Keras Deep Learn model for Multi-label Text Classification. Accomplishing the main objective of this project.\n \nThat was more painful than it seemed, haha. I have tried several times until find a good result with neural networks however fortunately I found  the [Python for NLP: Multi-label Text Classification with Keras](https://stackabuse.com/python-for-nlp-multi-label-text-classification-with-keras/) article that helps me a lot! I really want to thank the [Usman Malik](https://twitter.com/usman_malikk), for writing it. That was essential to accomplish that result in this project. \n \nI'm open to all improvements, rewording or criticism. Please don't hesitate to leave me a comment and If you liked this notebook, please upvote and leave your feedback in the comments!\n \nIt motivates me a lot to keep doing those types of projects and sharing here.\n \nThat's it for now.\n \n \nCheers üçÄ\n\n[Back To Top](#top)\n"},{"metadata":{"papermill":{"duration":1.043081,"end_time":"2021-03-08T18:29:21.722137","exception":false,"start_time":"2021-03-08T18:29:20.679056","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}